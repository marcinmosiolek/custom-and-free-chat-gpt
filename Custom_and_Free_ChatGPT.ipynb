{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "OpenAI has just announced and released [plugins for ChatGPT](https://openai.com/blog/chatgpt-plugins) that address its key problems - hallucinations and difficulties in updating with new, case-specific information. The plugins work mainly by restricting the generation of answers to a specific context, obtained from a dedicated information source, such as your specific documents.\n",
        "\n",
        "Overcoming these challenges makes ChatGPT suitable for any application. However, it also comes at a significant cost. Fortunately, there is an open source alternative that can be prototyped in 15 minutes. In this short article, I'll show how to do retrieved augmented generative question answering with [Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html) and [Sentence Transformers](https://www.sbert.net). That is, how to construct a solution that answers your questions like a human domain expert.\n"
      ],
      "metadata": {
        "id": "-ttpVpu_XBrd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Installing dependecies\n",
        "As the very first step we need to install the required python dependecies"
      ],
      "metadata": {
        "id": "LNVW9lnSX1o8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40oYRzLq5D87"
      },
      "outputs": [],
      "source": [
        "!pip install bitsandbytes datasets loralib sentencepiece \n",
        "!pip install git+https://github.com/huggingface/transformers.git\n",
        "!pip install git+https://github.com/huggingface/peft.git tenacity\n",
        "!pip install -U sentence-transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Alpaca - Instruction-Following Language Model\n",
        "\n",
        "[Stanford's Alpaca7B](https://https://crfm.stanford.edu/2023/03/13/alpaca.html) is a small but super powerful instruction-following language model constructed in a very clever way.  In other words, it works just like ChatGPT, but it's much smaller and free to use - you can even [run it on your CPU](https://github.com/antimatter15/alpaca.cpp)!"
      ],
      "metadata": {
        "id": "6Q7py6QoGHYk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from transformers import GenerationConfig\n",
        "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
        "from peft import PeftModel\n",
        "import bitsandbytes as bnb\n",
        "\n",
        "# The code comes from here: https://github.com/deep-diver/Alpaca-LoRA-Serve/\n",
        "\n",
        "GENERATION_CONFIG = GenerationConfig(\n",
        "    max_lenght=256,\n",
        "    temperature=0.9,\n",
        "    top_p=0.75,\n",
        "    num_beams=1,\n",
        "    use_cache=True,\n",
        "    min_length=0\n",
        ")\n",
        "\n",
        "def load_model(\n",
        "    base=\"decapoda-research/llama-7b-hf\",\n",
        "    finetuned=\"tloen/alpaca-lora-7b\",\n",
        "):\n",
        "    tokenizer = LlamaTokenizer.from_pretrained(base)\n",
        "    tokenizer.pad_token_id = 0\n",
        "    tokenizer.padding_side = \"left\"\n",
        "\n",
        "    model = LlamaForCausalLM.from_pretrained(\n",
        "        base,\n",
        "    )\n",
        "    \n",
        "    model = PeftModel.from_pretrained(model, finetuned).to(\"cuda\")\n",
        "    return model, tokenizer\n",
        "\n",
        "model, tokenizer = load_model()"
      ],
      "metadata": {
        "id": "Rjfn5RTuYwgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Semantic Search\n",
        "\n",
        "The idea of semantic search is to embed the meaning of the text in a vector of numbers and then use this vector to search for similar documents - vector similarity indicates the similarity of documents. The embedding is done using deep learning models specially trained for the task of semantic similarity, while the searching is done using vector search databases. However, in this example we will limit our solution to what comes with the Python package called [SentenceTransformers](https://sbert.net).\n",
        "\n",
        "For simplicity, we'll use simple-wiki as our search database. We will also download the corresponding pre-computed text embeddings of the dataset to save time and resources. This way we only need to compute the embeddings of the query.\n"
      ],
      "metadata": {
        "id": "1ZlK96YyYz2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gzip\n",
        "import json\n",
        "\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "\n",
        "# This code comes from: https://github.com/UKPLab/sentence-transformers/\n",
        "\n",
        "def load_wikipedia():\n",
        "  wikipedia_filepath = 'data/simplewiki-2020-11-01.jsonl.gz'\n",
        "\n",
        "  # retrieve the dataset from online location\n",
        "  if not os.path.exists(wikipedia_filepath):\n",
        "    util.http_get('http://sbert.net/datasets/simplewiki-2020-11-01.jsonl.gz', wikipedia_filepath)\n",
        "\n",
        "  # extract the text and store as a list in passages variable\n",
        "  passages = []\n",
        "  with gzip.open(wikipedia_filepath, 'rt', encoding='utf8') as fIn:\n",
        "    for line in fIn:\n",
        "        data = json.loads(line.strip())\n",
        "        for paragraph in data['paragraphs']:\n",
        "            # We encode the passages as [title, text]\n",
        "            passages.append([data['title'], paragraph])\n",
        "\n",
        "  # also download the embeddings to avoid redunant computation\n",
        "  embeddings_filepath = 'simplewiki-2020-11-01-nq-distilbert-base-v1.pt'\n",
        "  if not os.path.exists(embeddings_filepath):\n",
        "    util.http_get('http://sbert.net/datasets/simplewiki-2020-11-01-nq-distilbert-base-v1.pt', embeddings_filepath)\n",
        "\n",
        "  corpus_embeddings = torch.load(embeddings_filepath)\n",
        "  corpus_embeddings = corpus_embeddings.float()  # Convert embedding file to float\n",
        "  if torch.cuda.is_available():\n",
        "      corpus_embeddings = corpus_embeddings.to('cuda')\n",
        "\n",
        "  return passages, corpus_embeddings\n",
        "\n",
        "# load the dataset\n",
        "passages, embeddings = load_wikipedia()\n",
        "\n",
        "# load the embeddings model to be used for the query\n",
        "model_name = 'nq-distilbert-base-v1'\n",
        "bi_encoder = SentenceTransformer(model_name)"
      ],
      "metadata": {
        "id": "bVEZ6aJ-XBAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### Domain Specific Generative Question Answering\n",
        "\n",
        "What we are now doing is combining semantic search with generative question answering. First, we will look up the answer to our question in the database and retrieve the correct document. We need to embed the query into a vector and run the search. "
      ],
      "metadata": {
        "id": "4BHFP1VtZxKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"How many James Bond films has Sean Connery starred in??\"\n",
        "\n",
        "question_embedding = bi_encoder.encode(question, convert_to_tensor=True)\n",
        "hit = util.semantic_search(question_embedding, embeddings, top_k=1)\n",
        "retrieved_passage = passages[hit[0][0][\"corpus_id\"]][1]\n",
        "\n",
        "print(retrieved_passage)"
      ],
      "metadata": {
        "id": "fvXVb7QeaA5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we will ask Alpaca to answer our question by looking at the identified document. To do this we need to construct the correct prompt.\n"
      ],
      "metadata": {
        "id": "f29p-jIvaBFa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def answer(question, context, model, tokenizer):\n",
        "  query = [\n",
        "      \"Answer the question using the following context\\n\"\n",
        "      f\"Question: {question}\\n\"\n",
        "      f\"Context: {context}\"\n",
        "  ]\n",
        "\n",
        "  encodings = tokenizer(query, padding=True, return_tensors=\"pt\").to('cuda')\n",
        "  generated_ids = model.generate(\n",
        "    **encodings,\n",
        "    generation_config=GENERATION_CONFIG,\n",
        "    max_new_tokens=256\n",
        "  )\n",
        "\n",
        "  decoded = tokenizer.batch_decode(generated_ids)\n",
        "  del encodings, generated_ids\n",
        "  torch.cuda.empty_cache()\n",
        "  return decoded[0].split(\"\\n\")[-1]"
      ],
      "metadata": {
        "id": "5hKWNdUdaHWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And voila! That's it:"
      ],
      "metadata": {
        "id": "H7Xg1qvfao2U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_question(question):\n",
        "  question_embedding = bi_encoder.encode(question, convert_to_tensor=True)\n",
        "  hit = util.semantic_search(question_embedding, embeddings, top_k=1)\n",
        "  retrieved_passage = passages[hit[0][0][\"corpus_id\"]][1]\n",
        "\n",
        "  return answer(question, retrieved_passage, model, tokenizer)"
      ],
      "metadata": {
        "id": "gmD0OPNl5I_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let' see some results:"
      ],
      "metadata": {
        "id": "f_1KqcIBa5pk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answer_question(\"How many James Bond films has Sean Connery starred in?\")"
      ],
      "metadata": {
        "id": "0PaWqJIdA8Kb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer_question(\"Who played Vito Corleone in the movie Godfather?\")"
      ],
      "metadata": {
        "id": "llPz4TqjAz8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer_question(\"What is the most popular album of Pink Floyd?\")"
      ],
      "metadata": {
        "id": "xGDJyNwLbH9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer_question(\"What reason of establishing the city of Gdynia?\")"
      ],
      "metadata": {
        "id": "pnoltaF4fP3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Summary\n",
        "\n",
        "Obviously, the above example is oversimplified and much more work is needed to make it part of commercial software. For example, a [larger model could be used to better follow user instructions](https://https://huggingface.co/baseten/alpaca-30b), different prompts could be evaluated, extend the context to more than a single document, add conversation history and finally the semantic search could be performed by more sophisticated models and vector databases. But the idea remains the same."
      ],
      "metadata": {
        "id": "Ie997d2JbO5q"
      }
    }
  ]
}