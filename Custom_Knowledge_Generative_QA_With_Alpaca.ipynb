{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOY7NtgKORSUhtv9y18i3XG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium",
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c866ad95d5b548f69aaf7d2f165e1f0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d84b0efc869640c69e9ab511e8aa7dcd",
              "IPY_MODEL_98223e9f0a8244938c866611714cef49",
              "IPY_MODEL_ca79605758c841c2b67d8c6b4a6c90ab"
            ],
            "layout": "IPY_MODEL_d97b019a3aae46e08aa0ba3e5f996332"
          }
        },
        "d84b0efc869640c69e9ab511e8aa7dcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_87ba7f083d73445a89cf478893f734a9",
            "placeholder": "​",
            "style": "IPY_MODEL_a61f4a7f2d5440b48d965211c20fef17",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "98223e9f0a8244938c866611714cef49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b477083f34494771a8cda22f0f8beaca",
            "max": 33,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_50f5c5bb8d5f426eae6f6387906c6e5f",
            "value": 33
          }
        },
        "ca79605758c841c2b67d8c6b4a6c90ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2808cd01c8a04db885d2a7ec9e4b0709",
            "placeholder": "​",
            "style": "IPY_MODEL_29a548406e714cb7815c9c141f97875c",
            "value": " 33/33 [00:12&lt;00:00,  2.62it/s]"
          }
        },
        "d97b019a3aae46e08aa0ba3e5f996332": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87ba7f083d73445a89cf478893f734a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a61f4a7f2d5440b48d965211c20fef17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b477083f34494771a8cda22f0f8beaca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50f5c5bb8d5f426eae6f6387906c6e5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2808cd01c8a04db885d2a7ec9e4b0709": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29a548406e714cb7815c9c141f97875c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marcinmosiolek/custom-and-free-chat-gpt/blob/main/Custom_Knowledge_Generative_QA_With_Alpaca.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "OpenAI has just announced and released [plugins for ChatGPT](https://openai.com/blog/chatgpt-plugins) that address its key problems - hallucinations and difficulties in updating with new, case-specific information. The plugins work mainly by restricting the generation of answers to a specific context, obtained from a dedicated information source, such as your specific documents.\n",
        "\n",
        "Overcoming these challenges makes ChatGPT suitable for any application. However, it also comes at a significant cost. Fortunately, there is an open source alternative that can be prototyped in 15 minutes. In this short article, I'll show how to do retrieved augmented generative question answering with [Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html) and [Sentence Transformers](https://www.sbert.net). That is, how to construct a solution that answers your questions like a human domain expert.\n"
      ],
      "metadata": {
        "id": "-ttpVpu_XBrd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Installing dependecies\n",
        "As the very first step we need to install the required python dependecies"
      ],
      "metadata": {
        "id": "LNVW9lnSX1o8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40oYRzLq5D87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86e8d2b6-f905-44c5-e22a-2bb362af0a79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q bitsandbytes datasets loralib sentencepiece tenacity\n",
        "!pip install -q git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q git+https://github.com/huggingface/peft.git\n",
        "!pip install -q sentence-transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Alpaca - Instruction-Following Language Model\n",
        "\n",
        "[Stanford's Alpaca7B](https://https://crfm.stanford.edu/2023/03/13/alpaca.html) is a small but super powerful instruction-following language model constructed in a [very clever way](https://arxiv.org/abs/2212.10560). In other words, it works just like ChatGPT, but it's much smaller and free to use - you can even [run it on your CPU](https://github.com/antimatter15/alpaca.cpp)!"
      ],
      "metadata": {
        "id": "6Q7py6QoGHYk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from transformers import GenerationConfig\n",
        "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
        "from peft import PeftModel\n",
        "import bitsandbytes as bnb\n",
        "\n",
        "# The code comes from here: https://github.com/deep-diver/Alpaca-LoRA-Serve/\n",
        "\n",
        "GENERATION_CONFIG = GenerationConfig(\n",
        "    max_lenght=256,\n",
        "    temperature=0.9,\n",
        "    top_p=0.75,\n",
        "    num_beams=1,\n",
        "    use_cache=True,\n",
        "    min_length=0\n",
        ")\n",
        "\n",
        "\n",
        "def load_model(\n",
        "        base=\"decapoda-research/llama-7b-hf\",\n",
        "        finetuned=\"tloen/alpaca-lora-7b\",\n",
        "):\n",
        "    tokenizer = LlamaTokenizer.from_pretrained(base)\n",
        "    tokenizer.pad_token_id = 0\n",
        "    tokenizer.padding_side = \"left\"\n",
        "\n",
        "    model = LlamaForCausalLM.from_pretrained(\n",
        "        base,\n",
        "    )\n",
        "\n",
        "    model = PeftModel.from_pretrained(model, finetuned).to(\"cuda\")\n",
        "    return model, tokenizer\n",
        "\n",
        "\n",
        "model, tokenizer = load_model()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443,
          "referenced_widgets": [
            "c866ad95d5b548f69aaf7d2f165e1f0c",
            "d84b0efc869640c69e9ab511e8aa7dcd",
            "98223e9f0a8244938c866611714cef49",
            "ca79605758c841c2b67d8c6b4a6c90ab",
            "d97b019a3aae46e08aa0ba3e5f996332",
            "87ba7f083d73445a89cf478893f734a9",
            "a61f4a7f2d5440b48d965211c20fef17",
            "b477083f34494771a8cda22f0f8beaca",
            "50f5c5bb8d5f426eae6f6387906c6e5f",
            "2808cd01c8a04db885d2a7ec9e4b0709",
            "29a548406e714cb7815c9c141f97875c"
          ]
        },
        "id": "Rjfn5RTuYwgp",
        "outputId": "0ff63fc5-d301-4fd4-ec87-90596b78edce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /usr/local/lib/python3.9/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: /usr/lib64-nvidia did not contain libcudart.so as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-a100-s-3gk9lmlrd0phw --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true'), PosixPath('--listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/env/python')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//ipykernel.pylab.backend_inline')}\n",
            "  warn(msg)\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
            "The class this function is called from is 'LlamaTokenizer'.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c866ad95d5b548f69aaf7d2f165e1f0c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Semantic Search\n",
        "\n",
        "The idea of semantic search is to embed the meaning of the text in a vector of numbers and then use this vector to search for similar documents - vector similarity indicates the similarity of documents. The embedding is done using deep learning models specially trained for the task of semantic similarity, while the searching is done using vector search databases. However, in this example we will limit our solution to what comes with the Python package called [SentenceTransformers](https://sbert.net).\n",
        "\n",
        "For simplicity, we'll use simple-wiki to resemble a custom dataset. We will also download the corresponding pre-computed text embeddings of the dataset to save time and resources. This way we only need to compute the embeddings of the query.\n"
      ],
      "metadata": {
        "id": "1ZlK96YyYz2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gzip\n",
        "import json\n",
        "\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "\n",
        "# This code comes from: https://github.com/UKPLab/sentence-transformers/\n",
        "\n",
        "def load_wikipedia():\n",
        "    wikipedia_filepath = 'data/simplewiki-2020-11-01.jsonl.gz'\n",
        "\n",
        "    # retrieve the dataset from online location\n",
        "    if not os.path.exists(wikipedia_filepath):\n",
        "        util.http_get('http://sbert.net/datasets/simplewiki-2020-11-01.jsonl.gz', wikipedia_filepath)\n",
        "\n",
        "    # extract the text and store as a list in passages variable\n",
        "    passages = []\n",
        "    with gzip.open(wikipedia_filepath, 'rt', encoding='utf8') as fIn:\n",
        "        for line in fIn:\n",
        "            data = json.loads(line.strip())\n",
        "            for paragraph in data['paragraphs']:\n",
        "                # We encode the passages as [title, text]\n",
        "                passages.append([data['title'], paragraph])\n",
        "\n",
        "    # also download the embeddings to avoid redunant computation\n",
        "    embeddings_filepath = 'simplewiki-2020-11-01-nq-distilbert-base-v1.pt'\n",
        "    if not os.path.exists(embeddings_filepath):\n",
        "        util.http_get('http://sbert.net/datasets/simplewiki-2020-11-01-nq-distilbert-base-v1.pt', embeddings_filepath)\n",
        "\n",
        "    corpus_embeddings = torch.load(embeddings_filepath)\n",
        "    corpus_embeddings = corpus_embeddings.float()  # Convert embedding file to float\n",
        "    if torch.cuda.is_available():\n",
        "        corpus_embeddings = corpus_embeddings.to('cuda')\n",
        "\n",
        "    return passages, corpus_embeddings\n",
        "\n",
        "\n",
        "# load the dataset\n",
        "passages, embeddings = load_wikipedia()\n",
        "\n",
        "# load the embeddings model to be used for the query\n",
        "model_name = 'nq-distilbert-base-v1'\n",
        "bi_encoder = SentenceTransformer(model_name)"
      ],
      "metadata": {
        "id": "bVEZ6aJ-XBAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### Domain Specific Generative Question Answering\n",
        "\n",
        "What we are now doing is combining semantic search with generative question answering. First, we will look up the answer to our question in the database and retrieve the correct document. We need to embed the query into a vector and run the search. "
      ],
      "metadata": {
        "id": "4BHFP1VtZxKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"How many James Bond films has Sean Connery starred in??\"\n",
        "\n",
        "question_embedding = bi_encoder.encode(question, convert_to_tensor=True)\n",
        "hit = util.semantic_search(question_embedding, embeddings, top_k=1)\n",
        "retrieved_passage = passages[hit[0][0][\"corpus_id\"]][1]\n",
        "\n",
        "print(retrieved_passage)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvXVb7QeaA5Z",
        "outputId": "a5c077f3-0f46-447d-e639-c3f1f913b08d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sir Thomas Sean Connery (25 August 1930 – 31 October 2020) was a Scottish actor. He was known for his charm and good looks, which have made him very famous. He was best known for playing James Bond in seven of the James Bond movies. He appeared in 94 movies. He won the Academy Award for Best Supporting Actor for his role as Jimmy Malone in \"The Untouchables\" (1987).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we will ask Alpaca to answer our question by looking at the identified document. To do this we need to construct the correct prompt.\n"
      ],
      "metadata": {
        "id": "f29p-jIvaBFa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def answer(question, context, model, tokenizer):\n",
        "    query = [\n",
        "        \"Answer the question using the following context\\n\"\n",
        "        f\"Question: {question}\\n\"\n",
        "        f\"Context: {context}\"\n",
        "    ]\n",
        "\n",
        "    encodings = tokenizer(query, padding=True, return_tensors=\"pt\").to('cuda')\n",
        "    generated_ids = model.generate(\n",
        "        **encodings,\n",
        "        generation_config=GENERATION_CONFIG,\n",
        "        max_new_tokens=256\n",
        "    )\n",
        "\n",
        "    decoded = tokenizer.batch_decode(generated_ids)\n",
        "    del encodings, generated_ids\n",
        "    torch.cuda.empty_cache()\n",
        "    return decoded[0].split(\"\\n\")[-1]"
      ],
      "metadata": {
        "id": "5hKWNdUdaHWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And voila! That's it:"
      ],
      "metadata": {
        "id": "H7Xg1qvfao2U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_question(question):\n",
        "    question_embedding = bi_encoder.encode(question, convert_to_tensor=True)\n",
        "    hit = util.semantic_search(question_embedding, embeddings, top_k=1)\n",
        "    retrieved_passage = passages[hit[0][0][\"corpus_id\"]][1]\n",
        "\n",
        "    return answer(question, retrieved_passage, model, tokenizer)"
      ],
      "metadata": {
        "id": "gmD0OPNl5I_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let' see some results:"
      ],
      "metadata": {
        "id": "f_1KqcIBa5pk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answer_question(\"How many James Bond films has Sean Connery starred in?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0PaWqJIdA8Kb",
        "outputId": "b977c221-63aa-4492-969f-d8732762311f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Answer: Sean Connery starred in seven James Bond films.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer_question(\"Who played Vito Corleone in the movie Godfather?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "llPz4TqjAz8I",
        "outputId": "5386112d-bef1-43ea-def6-c3e0f090e7e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Answer: Marlon Brando'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer_question(\"What is the most popular album of Pink Floyd?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "xGDJyNwLbH9s",
        "outputId": "73b8c1ff-d24c-4d90-842d-f40864cb962b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Answer: The most popular album of Pink Floyd is The Dark Side of the Moon. It was released in 1973 and has sold over 45 million copies worldwide. It is the second best-selling album of all time, behind Michael Jackson's Thriller.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer_question(\"What reason of establishing the city of Gdynia?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "pnoltaF4fP3E",
        "outputId": "cd321890-6bc6-4a84-9bca-c94fac8fa7ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Answer: The city of Gdynia was established in 1926 as a port city to serve the needs of the growing Polish economy.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Summary\n",
        "\n",
        "Obviously, the above example is oversimplified and much more work is needed to make it part of commercial software. For example, a [larger model could be used to better follow user instructions](https://https://huggingface.co/baseten/alpaca-30b), different prompts could be evaluated, extend the context to more than a single document, add conversation history and finally the semantic search could be performed by more sophisticated models and vector databases. But the idea remains the same."
      ],
      "metadata": {
        "id": "Ie997d2JbO5q"
      }
    }
  ]
}